{
  "repo": "https://github.com/Yakvenalex/LangGraphExample",
  "status": "PARTIAL_SUCCESS",
  "exit_code": 1,
  "entry_point": "/tmp/repo/topic_1/llm_init_example.py",
  "tier": 1,
  "event_count": 8,
  "duration_seconds": 267,
  "run_id": "e714e75b3102409b",
  "vllm_model": "mistralai/Mistral-7B-Instruct-v0.3",
  "error_log_tail": "    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1131, in _generate\n    response = self.client.create(**payload)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/openai_patch.py\", line 238, in wrapper\n    result = original(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1087, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1256, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1044, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.AuthenticationError: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
}