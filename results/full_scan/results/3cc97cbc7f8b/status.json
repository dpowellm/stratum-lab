{
  "repo": "https://github.com/uebelack/bug-bounty-hunting-ai",
  "status": "PARTIAL_SUCCESS",
  "exit_code": 1,
  "entry_point": "/tmp/repo/src/simple-agent.py",
  "tier": 1,
  "event_count": 11,
  "duration_seconds": 147,
  "run_id": "20b9d4262a194979",
  "vllm_model": "mistralai/Mistral-7B-Instruct-v0.3",
  "error_log_tail": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/anthropic_patch.py\", line 351, in _make_openai_call\n    return client.chat.completions.create(**call_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/openai_patch.py\", line 238, in wrapper\n    result = original(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1297, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1070, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 64000. This model's maximum context length is 4096 tokens and your request has 745 input tokens (64000 > 4096 - 745). (parameter=max_tokens, value=64000)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'agent' and id 'db42325d-ca17-2e21-3f92-45ad94cdfaff'\n"
}