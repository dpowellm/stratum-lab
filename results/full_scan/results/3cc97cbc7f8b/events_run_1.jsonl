{"event_id": "evt_2fdaa0e70da94bf3", "timestamp_ns": 1771378592981780114, "run_id": "cf0ff59c-27e0-4552-9344-1c04c692f523", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_31ddc6c6991d4ea6", "timestamp_ns": 1771378605482467688, "run_id": "cf0ff59c-27e0-4552-9344-1c04c692f523", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_54d6ce67c08a4e37", "timestamp_ns": 1771378618254826504, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_0ae1b0d369044fff", "timestamp_ns": 1771378633965457706, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_208e52b6f66a44f5", "timestamp_ns": 1771378667224254503, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_8a02592161344abf", "timestamp_ns": 1771378722064195277, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_1b6bfbebceb448f5", "timestamp_ns": 1771378722876562171, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "llm.call_start", "source_node": {"node_type": "capability", "node_id": "anthropic:Messages:/usr/local/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:1396", "node_name": "anthropic.messages.create"}, "payload": {"model_requested": "claude-sonnet-4-20250514", "model_actual": "mistralai/Mistral-7B-Instruct-v0.3", "model_mapped": true, "message_count": 1, "has_tools": true, "redirected_to": "vllm_openai_compat"}, "stack_depth": 0}
{"event_id": "evt_fb7c7d7e19ea4aa3", "timestamp_ns": 1771378722898842026, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "llm.call_start", "source_node": {"node_type": "capability", "node_id": "openai:ChatCompletion:/opt/stratum_patcher/anthropic_patch.py:415", "node_name": "openai.chat.completions.create"}, "payload": {"model_requested": "mistralai/Mistral-7B-Instruct-v0.3", "model_actual": "mistralai/Mistral-7B-Instruct-v0.3", "model_mapped": false, "message_count": 2, "has_tools": true, "system_prompt_preview": "\nYou are my personal bug bounty hunting assistant. You will help me find vulnerabilities in web applications.\n", "system_prompt_hash": "dcff5e2c8587e949", "last_user_message_preview": "\nTry to find vulnerabilities in the following web application: http://localhost:3000/\n", "last_user_message_hash": "dbf1ced945a7c45d"}, "stack_depth": 0}
{"event_id": "evt_d5505a478b96420d", "timestamp_ns": 1771378723113475516, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "llm.call_end", "source_node": {"node_type": "capability", "node_id": "openai:ChatCompletion:/opt/stratum_patcher/anthropic_patch.py:415", "node_name": "openai.chat.completions.create"}, "payload": {"model_requested": "mistralai/Mistral-7B-Instruct-v0.3", "latency_ms": 214.37, "error": "Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 64000. This model's maximum context length is 4096 tokens and your request has 745 input tokens (64000 > 4096 - 745). (parameter=max_tokens, value=64000)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "error_type": "runtime_error", "active_node_stack": ["openai:ChatCompletion:/opt/stratum_patcher/anthropic_patch.py:415"]}, "parent_event_id": "evt_fb7c7d7e19ea4aa3", "stack_depth": 0}
{"event_id": "evt_232ab75a49df4c4f", "timestamp_ns": 1771378723113704189, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "llm.call_end", "source_node": {"node_type": "capability", "node_id": "anthropic:Messages:/usr/local/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:1396", "node_name": "anthropic.messages.create"}, "payload": {"model_requested": "claude-sonnet-4-20250514", "latency_ms": 236.92, "redirected_to": "vllm_openai_compat", "error": "Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 64000. This model's maximum context length is 4096 tokens and your request has 745 input tokens (64000 > 4096 - 745). (parameter=max_tokens, value=64000)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}"}, "parent_event_id": "evt_1b6bfbebceb448f5", "stack_depth": 0}
{"event_id": "evt_ac9c1a8839214717", "timestamp_ns": 1771378723123238541, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "error.occurred", "source_node": {"node_type": "agent", "node_id": "generic:exception:/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1070", "node_name": "unhandled_exception"}, "payload": {"error_type": "BadRequestError", "error_message": "Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 64000. This model's maximum context length is 4096 tokens and your request has 745 input tokens (64000 > 4096 - 745). (parameter=max_tokens, value=64000)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "traceback_tail": "on3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1297, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1070, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 64000. This model's maximum context length is 4096 tokens and your request has 745 input tokens (64000 > 4096 - 745). (parameter=max_tokens, value=64000)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'agent' and id 'db42325d-ca17-2e21-3f92-45ad94cdfaff'\n", "file": "/usr/local/lib/python3.11/site-packages/openai/_base_client.py", "line": 1070}, "stack_depth": 0}
{"event_id": "evt_f04d5c40637345e2", "timestamp_ns": 1771378734853044653, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_44c21878ae43465c", "timestamp_ns": 1771378734854012257, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "file.read", "source_node": {"node_type": "data_store", "node_id": "generic:file_io:/app/output/stderr.log:0", "node_name": "/app/output/stderr.log"}, "payload": {"path": "/app/output/stderr.log", "mode": "r"}, "stack_depth": 0}
{"event_id": "evt_ceb7dff641a34be0", "timestamp_ns": 1771378734854299298, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "file.write", "source_node": {"node_type": "data_store", "node_id": "generic:file_io:/app/output/status.json:0", "node_name": "/app/output/status.json"}, "payload": {"path": "/app/output/status.json", "mode": "w"}, "stack_depth": 0}
{"event_id": "evt_23e0f22f9d054ed4", "timestamp_ns": 1771378747348136841, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "patcher.status", "payload": {"patches": {"generic": "ok", "openai": "ok", "anthropic": "ok", "crewai": "ok", "langgraph": "ok", "autogen": "ok", "litellm": "ok"}, "patches_ok": 7, "patches_skipped": 0}, "stack_depth": 0}
{"event_id": "evt_73bca6642bf844ae", "timestamp_ns": 1771378747349202028, "run_id": "20b9d4262a194979", "repo_id": "https://github.com/uebelack/bug-bounty-hunting-ai", "framework": "auto", "event_type": "file.write", "source_node": {"node_type": "data_store", "node_id": "generic:file_io:/app/output/run_metadata_1.json:0", "node_name": "/app/output/run_metadata_1.json"}, "payload": {"path": "/app/output/run_metadata_1.json", "mode": "w"}, "stack_depth": 0}
