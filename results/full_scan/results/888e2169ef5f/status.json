{
  "repo": "https://github.com/Mar-Issah/langchain-agents",
  "status": "PARTIAL_SUCCESS",
  "exit_code": 1,
  "entry_point": "/tmp/repo/simple_chatbot.py",
  "tier": 1,
  "event_count": 12,
  "duration_seconds": 105,
  "run_id": "b14be374f59c42d9",
  "vllm_model": "mistralai/Mistral-7B-Instruct-v0.3",
  "error_log_tail": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/anthropic_patch.py\", line 351, in _make_openai_call\n    return client.chat.completions.create(**call_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/openai_patch.py\", line 238, in wrapper\n    result = original(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1297, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1070, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 4096 tokens and your request has 13 input tokens (4096 > 4096 - 13). (parameter=max_tokens, value=4096)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'chatbot' and id 'aba76635-66ef-c2a9-96c7-95b98365897a'\n"
}