{
  "repo": "https://github.com/aimanyounises1/LangChainTutorial",
  "status": "PARTIAL_SUCCESS",
  "exit_code": 1,
  "entry_point": "/tmp/repo/langgraph_examples/deep_research_agent/deep_research_simplified.py",
  "tier": 1,
  "event_count": 18,
  "duration_seconds": 333,
  "run_id": "b5509f20a81147ac",
  "vllm_model": "mistralai/Mistral-7B-Instruct-v0.3",
  "error_log_tail": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/anthropic_patch.py\", line 351, in _make_openai_call\n    return client.chat.completions.create(**call_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/openai_patch.py\", line 238, in wrapper\n    result = original(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1297, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1070, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your request has 6058 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=6058)\", 'type': 'BadRequestError', 'param': 'input_tokens', 'code': 400}}\nDuring task with name 'model' and id 'fff786d5-f5dd-702b-0fee-752f52baf666'\n"
}