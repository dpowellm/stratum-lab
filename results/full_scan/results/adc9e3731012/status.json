{
  "repo": "https://github.com/ruslanmv/Best-of-the-Best",
  "status": "PARTIAL_SUCCESS",
  "exit_code": 1,
  "entry_point": "/tmp/repo/multiagent_system/crew.py",
  "tier": 1,
  "event_count": 28,
  "duration_seconds": 363,
  "run_id": "18a30363387b46a1",
  "vllm_model": "mistralai/Mistral-7B-Instruct-v0.3",
  "error_log_tail": "             ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/crewai/llm.py\", line 164, in call\n    response = litellm.completion(**params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/stratum_patcher/litellm_patch.py\", line 97, in wrapper\n    result = original(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1748, in wrapper\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1569, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 4305, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2398, in exception_type\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 525, in exception_type\n    raise NotFoundError(\nlitellm.exceptions.NotFoundError: litellm.NotFoundError: NotFoundError: OpenAIException - The model `ollama/llama3.2` does not exist.\n"
}