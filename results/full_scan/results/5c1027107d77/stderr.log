ERROR:root:Context window exceeded: Error code: 400 - {'error': {'message': "This model's maximum context length is 4096 tokens. However, your request has 4506 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=4506)", 'type': 'BadRequestError', 'param': 'input_tokens', 'code': 400}}
ERROR:root:OpenAI API call failed: LLM context length exceeded. Original error: Error code: 400 - {'error': {'message': "This model's maximum context length is 4096 tokens. However, your request has 4506 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=4506)", 'type': 'BadRequestError', 'param': 'input_tokens', 'code': 400}}
Consider using a smaller input or implementing a text splitting strategy.
