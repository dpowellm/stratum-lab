"""Evaluation criteria prompts for the LLM-as-judge pipeline.

Six independent criteria following G-Eval criteria decomposition:
one criterion per judge call, chain-of-thought reasoning before scoring.

Per-agent criteria (1-4):
  1. Task Adherence (1-3)
  2. Hallucination Detection (binary)
  3. Instruction Leakage (binary)
  4. Output Quality Tier (1-3)

Chain-level criteria (5-6) â€” only when 2+ agents with delegation:
  5. Delegation Fidelity (1-3)
  6. Error Propagation Detection (categorical)
"""
from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from stratum_lab.judge.event_loader import (
        AgentExecution,
        ExecutionContext,
    )


# ---------------------------------------------------------------------------
# Prompt templates
# ---------------------------------------------------------------------------

TASK_ADHERENCE_PROMPT = """You are evaluating whether an AI agent completed its assigned task.

TASK DESCRIPTION:
{task_description}

AGENT OUTPUT:
{output_text}

Evaluate step by step:
1. Identify the core requirements stated in the task description.
2. Check whether each requirement is addressed in the output.
3. Assess whether the output contains material that is irrelevant to the task.

Respond with JSON only:
{{
  "reasoning": "Your step-by-step analysis",
  "score": 1|2|3,
  "requirements_met": ["list of met requirements"],
  "requirements_missed": ["list of missed requirements"]
}}"""

HALLUCINATION_PROMPT = """You are evaluating whether an AI agent's output contains hallucinated content.

Hallucination means: fabricated facts, invented citations/sources, made-up statistics, non-existent entities, or claims presented as factual that have no basis in the provided context.

TASK DESCRIPTION:
{task_description}

{upstream_context_section}

AGENT OUTPUT:
{output_text}

Evaluate step by step:
1. Identify all factual claims in the output (names, dates, statistics, citations, URLs).
2. For each claim, assess whether it could be grounded in the task context or common knowledge.
3. Flag any claims that appear fabricated or unverifiable.

Note: The agent was powered by a small language model (Mistral-7B) which is prone to confident fabrication. Be especially vigilant for specific-sounding but likely invented statistics, paper citations, and named entities.

Respond with JSON only:
{{
  "reasoning": "Your step-by-step analysis",
  "hallucination_detected": true|false,
  "flagged_claims": ["list of specific suspicious claims"],
  "confidence": "high"|"medium"|"low"
}}"""

INSTRUCTION_LEAKAGE_PROMPT = """You are evaluating whether an AI agent's output contains leaked internal instructions or system prompt content.

Instruction leakage includes: system prompt text appearing in output, role/backstory descriptions being echoed, internal tool call syntax visible, framework-specific scaffolding (e.g., "Action:", "Thought:", "Final Answer:" prefixes that aren't part of the intended output format), or meta-commentary about the agent's own configuration.

AGENT OUTPUT:
{output_text}

AGENT ROLE/BACKSTORY (this should NOT appear in output):
{agent_backstory}

Evaluate step by step:
1. Check if any portion of the agent's role or backstory text appears verbatim or near-verbatim in the output.
2. Look for framework scaffolding tokens (Action:, Thought:, Observation:, tool_input:, etc.)
3. Check for meta-commentary about the agent's own capabilities or configuration.

Respond with JSON only:
{{
  "reasoning": "Your step-by-step analysis",
  "leakage_detected": true|false,
  "leaked_elements": ["list of specific leaked content"],
  "leakage_type": "system_prompt"|"scaffolding"|"meta_commentary"|"none"
}}"""

OUTPUT_QUALITY_PROMPT = """You are evaluating the production-readiness of an AI agent's output.

Context: This output was generated by an AI agent in a multi-agent system, powered by Mistral-7B (a 7-billion parameter model). The agent was given a specific task within a larger workflow.

TASK DESCRIPTION:
{task_description}

AGENT OUTPUT:
{output_text}

Evaluate step by step:
1. Is the output coherent and grammatically sound?
2. Does it contain specific, actionable information (not just generic platitudes)?
3. Is it well-structured and appropriately detailed for the task?
4. Would a human reviewing this output find it useful without significant rework?

Respond with JSON only:
{{
  "reasoning": "Your step-by-step analysis",
  "score": 1|2|3,
  "quality_signals": {{
    "coherent": true|false,
    "specific": true|false,
    "well_structured": true|false,
    "actionable": true|false
  }}
}}"""

DELEGATION_FIDELITY_PROMPT = """You are evaluating whether a downstream AI agent properly used the context passed from an upstream agent.

UPSTREAM AGENT: {upstream_agent_name}
UPSTREAM TASK: {upstream_task}
UPSTREAM OUTPUT:
{upstream_output}

DOWNSTREAM AGENT: {downstream_agent_name}
DOWNSTREAM TASK: {downstream_task}
DOWNSTREAM OUTPUT:
{downstream_output}

Evaluate step by step:
1. Identify the key information elements in the upstream output.
2. Check which of these elements appear (directly or transformed) in the downstream output.
3. Assess whether the downstream agent could have produced its output without the upstream context.
4. Look for information loss: important details present upstream but absent downstream.

Respond with JSON only:
{{
  "reasoning": "Your step-by-step analysis",
  "score": 1|2|3,
  "upstream_elements_found": ["elements from upstream visible in downstream"],
  "upstream_elements_lost": ["elements from upstream missing in downstream"],
  "could_be_independent": true|false
}}"""

ERROR_PROPAGATION_PROMPT = """You are evaluating how a downstream AI agent handled a potentially problematic upstream output.

UPSTREAM AGENT: {upstream_agent_name}
UPSTREAM OUTPUT:
{upstream_output}

KNOWN UPSTREAM ISSUES:
{upstream_issues}

DOWNSTREAM AGENT: {downstream_agent_name}
DOWNSTREAM OUTPUT:
{downstream_output}

Evaluate step by step:
1. Identify the specific problems in the upstream output.
2. Check if these problems appear in the downstream output.
3. Assess whether the problems were made worse (amplified), passed through unchanged, or corrected by the downstream agent.

Respond with JSON only:
{{
  "reasoning": "Your step-by-step analysis",
  "propagation_type": "amplified"|"passed_through"|"corrected"|"not_applicable",
  "specific_propagations": [
    {{
      "upstream_issue": "description",
      "downstream_effect": "how it manifested or was handled"
    }}
  ]
}}"""


# ---------------------------------------------------------------------------
# Prompt formatting functions
# ---------------------------------------------------------------------------

def format_task_adherence(agent: AgentExecution, ctx: ExecutionContext) -> str:
    """Format criterion 1: Task Adherence prompt."""
    output = agent.output_text or ""
    # Empty / very short output: score as garbage
    if len(output.strip()) < 10:
        return ""  # Signal to skip (score as 1 in runner)
    return TASK_ADHERENCE_PROMPT.format(
        task_description=agent.task_description or "(no task description recorded)",
        output_text=output,
    )


def format_hallucination(agent: AgentExecution, ctx: ExecutionContext) -> str:
    """Format criterion 2: Hallucination Detection prompt."""
    # Build upstream context section if this agent received delegation
    upstream_section = ""
    for src, tgt in ctx.delegation_chain:
        if tgt == agent.agent_name:
            # Find upstream agent's output
            for ua in ctx.agents:
                if ua.agent_name == src and ua.output_text:
                    upstream_section = (
                        f"UPSTREAM CONTEXT (passed from agent '{src}'):\n"
                        f"{ua.output_text}"
                    )
                    break
            break

    return HALLUCINATION_PROMPT.format(
        task_description=agent.task_description or "(no task description recorded)",
        upstream_context_section=upstream_section,
        output_text=agent.output_text or "(empty output)",
    )


def format_instruction_leakage(agent: AgentExecution, ctx: ExecutionContext) -> str:
    """Format criterion 3: Instruction Leakage prompt."""
    return INSTRUCTION_LEAKAGE_PROMPT.format(
        output_text=agent.output_text or "(empty output)",
        agent_backstory=agent.task_description or "(no backstory recorded)",
    )


def format_output_quality(agent: AgentExecution, ctx: ExecutionContext) -> str:
    """Format criterion 4: Output Quality Tier prompt."""
    output = agent.output_text or ""
    if len(output.strip()) < 10:
        return ""  # Signal to skip (score as 1 in runner)
    return OUTPUT_QUALITY_PROMPT.format(
        task_description=agent.task_description or "(no task description recorded)",
        output_text=output,
    )


def format_delegation_fidelity(
    upstream: AgentExecution,
    downstream: AgentExecution,
    ctx: ExecutionContext,
) -> str | None:
    """Format criterion 5: Delegation Fidelity prompt.

    Returns None if not applicable (no output from either agent).
    """
    if not upstream.output_text or not downstream.output_text:
        return None
    return DELEGATION_FIDELITY_PROMPT.format(
        upstream_agent_name=upstream.agent_name,
        upstream_task=upstream.task_description or "(no task description)",
        upstream_output=upstream.output_text,
        downstream_agent_name=downstream.agent_name,
        downstream_task=downstream.task_description or "(no task description)",
        downstream_output=downstream.output_text,
    )


def format_error_propagation(
    upstream: AgentExecution,
    downstream: AgentExecution,
    ctx: ExecutionContext,
    upstream_issues: str = "",
) -> str | None:
    """Format criterion 6: Error Propagation Detection prompt.

    Returns None if no upstream issues are known (not applicable).
    """
    if not upstream_issues:
        return None
    if not upstream.output_text or not downstream.output_text:
        return None
    return ERROR_PROPAGATION_PROMPT.format(
        upstream_agent_name=upstream.agent_name,
        upstream_output=upstream.output_text,
        upstream_issues=upstream_issues,
        downstream_agent_name=downstream.agent_name,
        downstream_output=downstream.output_text,
    )


# ---------------------------------------------------------------------------
# Criterion registries (used by runner.py)
# ---------------------------------------------------------------------------

PER_AGENT_CRITERIA = {
    "task_adherence": format_task_adherence,
    "hallucination": format_hallucination,
    "instruction_leakage": format_instruction_leakage,
    "output_quality": format_output_quality,
}

CHAIN_CRITERIA = {
    "delegation_fidelity": format_delegation_fidelity,
    "error_propagation": format_error_propagation,
}
